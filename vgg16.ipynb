{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I use VGG16 as a feature-extractor for pictures of parcels taken in detroit. I put a simple fully-connected top on VGG16 which outputs the probability that the picture shows blight (0 = blighted, 1 = not_blighted). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Input, Dense, Dropout, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "import keras.backend as K\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, instantiate the VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are stored in a directory image_data. This has subdirectories train and test, each of which have subdirectories blighted and not_blighted. I'll use a Keras ImageGenerator class to pull images. For training, I'll allow zoomed images and flipped images so the net will rarely see the same image twice. For testing, I'll just show the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_datagen = image.ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_datagen = image.ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9301 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = train_datagen.flow_from_directory('image_data/train',\n",
    "                                              target_size=(224,224),\n",
    "                                              batch_size=128,\n",
    "                                              class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2326 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_gen = test_datagen.flow_from_directory('image_data/test',\n",
    "                                            target_size=(224,224),\n",
    "                                            batch_size=128,\n",
    "                                            class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll specify the top model interface and hook it up to my vgg16 instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.8))\n",
    "top_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(224,224,3))\n",
    "x = model(inputs)\n",
    "preds = top_model(x)\n",
    "combined_model = Model(input=inputs, output=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f78d5b4de48>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f78d630a3c8>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f78d52a3780>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f78d630a160>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f78d524ee48>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f78d5264e10>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f78d5278eb8>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f78d5212278>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f78d5230940>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f78d51ddac8>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f78d51fc940>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f78d51fcfd0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f78d5188c88>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f78d519b518>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f78d51af630>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f78d5150710>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f78d5171a20>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f78d511e0f0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f78d50c8e48>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_model.layers[1].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the we want to keep the layers of vgg16 frozen and only train the weights on the top model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for l in combined_model.layers[1].layers:\n",
    "    l.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to use precision, recall, fscore, and accuracy as my metrics. Keras currently implements accuracy but not the others so I'll define custom metrics and use these in compiling the combined_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fscore(y_true, y_pred, beta=1):\n",
    "    \"\"\"Computes the F score.\n",
    "\n",
    "    The F score is the weighted harmonic mean of precision and recall.\n",
    "    Here it is only computed as a batch-wise average, not globally.\n",
    "\n",
    "    This is useful for multi-label classification, where input samples can be\n",
    "    classified as sets of labels. By only using accuracy (precision) a model\n",
    "    would achieve a perfect score by simply assigning every class to every\n",
    "    input. In order to avoid this, a metric should penalize incorrect class\n",
    "    assignments as well (recall). The F-beta score (ranged from 0.0 to 1.0)\n",
    "    computes this, as a weighted mean of the proportion of correct class\n",
    "    assignments vs. the proportion of incorrect class assignments.\n",
    "\n",
    "    With beta = 1, this is equivalent to a F-measure. With beta < 1, assigning\n",
    "    correct classes becomes more important, and with beta > 1 the metric is\n",
    "    instead weighted towards penalizing incorrect class assignments.\n",
    "    \"\"\"\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_model.compile(loss='mse',\n",
    "                       optimizer='sgd',\n",
    "                       metrics=['accuracy', precision, recall, fscore])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train the combined model for 16 epochs, evaluating on the test data as we go along. This takes about 4 hours, about 15 minutes per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "908s - loss: 0.5142 - acc: 0.4790 - precision: 0.5263 - recall: 0.1291 - fscore: 0.1751 - val_loss: 0.4963 - val_acc: 0.5039 - val_precision: 0.5000 - val_recall: 0.0076 - val_fscore: 0.0149\n",
      "Epoch 2/16\n",
      "897s - loss: 0.4490 - acc: 0.5474 - precision: 0.6945 - recall: 0.1499 - fscore: 0.2409 - val_loss: 0.4815 - val_acc: 0.5156 - val_precision: 0.9722 - val_recall: 0.1027 - val_fscore: 0.1850\n",
      "Epoch 3/16\n",
      "886s - loss: 0.4190 - acc: 0.5752 - precision: 0.6540 - recall: 0.3293 - fscore: 0.4264 - val_loss: 0.3728 - val_acc: 0.6191 - val_precision: 0.6361 - val_recall: 0.5200 - val_fscore: 0.5720\n",
      "Epoch 4/16\n",
      "884s - loss: 0.4248 - acc: 0.5708 - precision: 0.6988 - recall: 0.2842 - fscore: 0.3898 - val_loss: 0.4586 - val_acc: 0.5391 - val_precision: 0.6726 - val_recall: 0.0449 - val_fscore: 0.0825\n",
      "Epoch 5/16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cathyw/miniconda3/lib/python3.5/site-packages/keras/engine/training.py:1480: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921s - loss: 0.4241 - acc: 0.5720 - precision: 0.7711 - recall: 0.1851 - fscore: 0.2916 - val_loss: 0.4127 - val_acc: 0.5861 - val_precision: 0.8428 - val_recall: 0.2137 - val_fscore: 0.3390\n",
      "Epoch 6/16\n",
      "885s - loss: 0.3992 - acc: 0.5952 - precision: 0.6671 - recall: 0.4182 - fscore: 0.5077 - val_loss: 0.3294 - val_acc: 0.6621 - val_precision: 0.6459 - val_recall: 0.5641 - val_fscore: 0.5975\n",
      "Epoch 7/16\n",
      "884s - loss: 0.4108 - acc: 0.5815 - precision: 0.5724 - recall: 0.6474 - fscore: 0.6022 - val_loss: 0.3705 - val_acc: 0.6230 - val_precision: 0.6929 - val_recall: 0.4294 - val_fscore: 0.5270\n",
      "Epoch 8/16\n",
      "884s - loss: 0.3867 - acc: 0.6074 - precision: 0.6300 - recall: 0.4976 - fscore: 0.5502 - val_loss: 0.3294 - val_acc: 0.6641 - val_precision: 0.6649 - val_recall: 0.7002 - val_fscore: 0.6807\n",
      "Epoch 9/16\n",
      "890s - loss: 0.3933 - acc: 0.6021 - precision: 0.6190 - recall: 0.6295 - fscore: 0.6182 - val_loss: 0.3445 - val_acc: 0.6442 - val_precision: 0.6006 - val_recall: 0.8306 - val_fscore: 0.6952\n",
      "Epoch 10/16\n",
      "913s - loss: 0.3905 - acc: 0.6029 - precision: 0.5823 - recall: 0.7274 - fscore: 0.6441 - val_loss: 0.3192 - val_acc: 0.6738 - val_precision: 0.6331 - val_recall: 0.8633 - val_fscore: 0.7298\n",
      "Epoch 11/16\n",
      "884s - loss: 0.3827 - acc: 0.6108 - precision: 0.5940 - recall: 0.7013 - fscore: 0.6412 - val_loss: 0.3062 - val_acc: 0.6855 - val_precision: 0.6406 - val_recall: 0.7969 - val_fscore: 0.7096\n",
      "Epoch 12/16\n",
      "884s - loss: 0.3650 - acc: 0.6318 - precision: 0.6025 - recall: 0.7388 - fscore: 0.6610 - val_loss: 0.2909 - val_acc: 0.7070 - val_precision: 0.6606 - val_recall: 0.8158 - val_fscore: 0.7287\n",
      "Epoch 13/16\n",
      "890s - loss: 0.3510 - acc: 0.6431 - precision: 0.6527 - recall: 0.6399 - fscore: 0.6424 - val_loss: 0.2713 - val_acc: 0.7154 - val_precision: 0.7719 - val_recall: 0.6173 - val_fscore: 0.6845\n",
      "Epoch 14/16\n",
      "913s - loss: 0.3328 - acc: 0.6606 - precision: 0.6786 - recall: 0.6506 - fscore: 0.6568 - val_loss: 0.2806 - val_acc: 0.7129 - val_precision: 0.7485 - val_recall: 0.5548 - val_fscore: 0.6361\n",
      "Epoch 15/16\n",
      "884s - loss: 0.3532 - acc: 0.6401 - precision: 0.6408 - recall: 0.6914 - fscore: 0.6602 - val_loss: 0.2818 - val_acc: 0.7109 - val_precision: 0.6705 - val_recall: 0.8432 - val_fscore: 0.7468\n",
      "Epoch 16/16\n",
      "884s - loss: 0.3512 - acc: 0.6426 - precision: 0.6359 - recall: 0.6627 - fscore: 0.6455 - val_loss: 0.2753 - val_acc: 0.7188 - val_precision: 0.7703 - val_recall: 0.6676 - val_fscore: 0.7135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f78d4ce9780>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_model.fit_generator(train_gen,\n",
    "                             samples_per_epoch=2048,\n",
    "                             nb_epoch=16,\n",
    "                             validation_data=test_gen,\n",
    "                             nb_val_samples=512,\n",
    "                             verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
